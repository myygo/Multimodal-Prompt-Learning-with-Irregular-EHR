#!/usr/bin/env python3
"""
[æœ€ç»ˆå®Œæ•´ç‰ˆ] ç”¨äºè®­ç»ƒ my_model.py ä¸­çš„ ICUPromptModelã€‚

åŠŸèƒ½äº®ç‚¹:
- å®ç°äº†ç²¾ç»†åŒ–å¾®è°ƒç­–ç•¥ (å‰Nä¸ªepochå¾®è°ƒï¼Œåç»­å†»ç»“)ã€‚
- å®ç°äº†æ¢¯åº¦ç´¯ç§¯ï¼Œä»¥åœ¨æœ‰é™æ˜¾å­˜ä¸‹æ¨¡æ‹Ÿå¤§æ‰¹é‡è®­ç»ƒã€‚
- å®ç°äº†å­é›†è®­ç»ƒæ¨¡å¼ï¼Œç”¨äºå¿«é€Ÿè°ƒè¯•å’ŒéªŒè¯ã€‚
- **æ–°å¢äº†æ¨¡æ€ä¸¢å¼ƒ (Modality Dropout) åŠŸèƒ½ï¼Œä»¥æå‡æ¨¡å‹é²æ£’æ€§ã€‚**
"""
from transformers import AutoModel, AutoTokenizer, AdamW, get_linear_schedule_with_warmup
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from sklearn.metrics import f1_score, average_precision_score
from torch.utils.data import DataLoader, Subset, RandomSampler, SequentialSampler
import os
import time
from tqdm import tqdm
import warnings
import random

warnings.filterwarnings('ignore')

# å¯¼å…¥æ‚¨é¡¹ç›®ä¸­çš„å¿…è¦æ¨¡å—
from model_prompt import create_icu_model
from data import data_perpare, TextTSIrgcollate_fn

class ICUTrainer:
    """å°è£…äº†è®­ç»ƒã€éªŒè¯å’Œæ£€æŸ¥ç‚¹ä¿å­˜çš„é€»è¾‘"""
    def __init__(self, model, device, optimizer, scheduler, class_weights=None, use_amp=True):
        self.model = model.to(device)
        self.device = device
        self.optimizer = optimizer
        self.scheduler = scheduler
        self.use_amp = use_amp and torch.cuda.is_available()

        self.update_param_count()
        
        if class_weights is not None:
            class_weights = class_weights.to(device)
        self.criterion = nn.CrossEntropyLoss(weight=class_weights)
        
        if self.use_amp:
            self.scaler = torch.cuda.amp.GradScaler()

    def update_param_count(self):
        params_to_train = [p for p in self.model.parameters() if p.requires_grad]
        print(f"  > æ¨¡å‹æ€»å‚æ•°é‡: {sum(p.numel() for p in self.model.parameters()):,}")
        print(f"  > å½“å‰å¯è®­ç»ƒå‚æ•°é‡: {sum(p.numel() for p in params_to_train):,}")

    # --- æ ¸å¿ƒä¿®æ”¹ 1: ä¸º train_epoch æ·»åŠ  modality_dropout_rate å‚æ•° ---
    def train_epoch(self, dataloader, gradient_accumulation_steps=1, modality_dropout_rate=0.0):
        self.model.train()
        total_loss = 0
        progress_bar = tqdm(dataloader, desc="Training")
        self.optimizer.zero_grad()
        
        for step, batch_data in enumerate(progress_bar):
            if batch_data is None: continue
            
            (ts_input_sequences, ts_mask_sequences, ts_tt, reg_ts_input,
             input_ids, attn_mask, note_time, note_time_mask, label) = batch_data

            # --- æ ¸å¿ƒä¿®æ”¹ 2: åœ¨è¿™é‡Œå®ç°æ¨¡æ€ä¸¢å¼ƒé€»è¾‘ ---
            if self.model.training and modality_dropout_rate > 0:
                if torch.rand(1).item() < modality_dropout_rate:
                    # å†³å®šä¸¢å¼ƒå“ªä¸ªæ¨¡æ€ (50/50 æ¦‚ç‡)
                    if torch.rand(1).item() < 0.5:
                        # ä¸¢å¼ƒæ–‡æœ¬æ¨¡æ€
                        input_ids, attn_mask, note_time, note_time_mask = [None] * 4
                    else:
                        # ä¸¢å¼ƒæ•°å€¼æ¨¡æ€
                        ts_input_sequences, ts_mask_sequences, ts_tt, reg_ts_input = [None] * 4
            
            # å°†æ•°æ®ç§»åŠ¨åˆ°è®¾å¤‡
            (ts_input_sequences, ts_mask_sequences, ts_tt, reg_ts_input,
             input_ids, attn_mask, note_time, note_time_mask, label) = [
                d.to(self.device) if torch.is_tensor(d) else None for d in 
                (ts_input_sequences, ts_mask_sequences, ts_tt, reg_ts_input,
                 input_ids, attn_mask, note_time, note_time_mask, label)
            ]

            with torch.cuda.amp.autocast(enabled=self.use_amp):
                outputs = self.model(
                    ts_input_sequences=ts_input_sequences, ts_mask_sequences=ts_mask_sequences, ts_tt=ts_tt,
                    reg_ts_input=reg_ts_input, input_ids=input_ids, attn_mask=attn_mask,
                    note_time=note_time, note_time_mask=note_time_mask, label=label
                )
                if outputs is None: continue
                loss = self.criterion(outputs, label)
                if gradient_accumulation_steps > 1:
                    loss = loss / gradient_accumulation_steps

            if self.use_amp:
                self.scaler.scale(loss).backward()
            else:
                loss.backward()
            
            total_loss += loss.item() * gradient_accumulation_steps
            
            if (step + 1) % gradient_accumulation_steps == 0 or (step + 1) == len(dataloader):
                if self.use_amp:
                    self.scaler.step(self.optimizer)
                    self.scaler.update()
                else:
                    self.optimizer.step()
                self.scheduler.step()
                self.optimizer.zero_grad()

            progress_bar.set_postfix({'Loss': f'{loss.item() * gradient_accumulation_steps:.4f}'}, refresh=True)

        return total_loss / len(progress_bar) if len(progress_bar) > 0 else 0

    def validate(self, dataloader):
        # ... (validate å‡½æ•°ä¿æŒä¸å˜) ...
        self.model.eval()
        all_preds, all_labels, all_probs = [], [], []
        
        with torch.no_grad():
            for batch_data in tqdm(dataloader, desc="Validating"):
                if batch_data is None: continue
                (ts_input_sequences, ts_mask_sequences, ts_tt, reg_ts_input,
                 input_ids, attn_mask, note_time, note_time_mask, label) = [
                    d.to(self.device) if torch.is_tensor(d) else None for d in batch_data
                ]
                outputs = self.model(
                    ts_input_sequences=ts_input_sequences, ts_mask_sequences=ts_mask_sequences, ts_tt=ts_tt,
                    reg_ts_input=reg_ts_input, input_ids=input_ids, attn_mask=attn_mask,
                    note_time=note_time, note_time_mask=note_time_mask, label=label
                )
                if outputs is None: continue
                probs = torch.softmax(outputs, dim=1)[:, 1]
                predicted = torch.argmax(outputs, dim=1)
                all_preds.extend(predicted.cpu().numpy())
                all_labels.extend(label.cpu().numpy())
                all_probs.extend(probs.cpu().numpy())
        
        if all_labels:
            f1 = f1_score(all_labels, all_preds, average='binary')
            auprc = average_precision_score(all_labels, all_probs)
            return f1, auprc
        return 0.0, 0.0

    def save_checkpoint(self, filepath, epoch, is_best=False):
        # ... (save_checkpoint å‡½æ•°ä¿æŒä¸å˜) ...
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        checkpoint = {'epoch': epoch, 'model_state_dict': self.model.state_dict()}
        if is_best:
            # --- åœ¨è¿™é‡Œä¿®æ”¹ä¸ºæ‚¨æƒ³è¦çš„æœ€ä½³æ¨¡å‹æ–‡ä»¶å ---
            # æˆ‘ä»¬ç›´æ¥ä½¿ç”¨ os.path.join æ¥æ„å»ºä¸€ä¸ªå…¨æ–°çš„è·¯å¾„ï¼Œæ›´å®‰å…¨
            dir_name = os.path.dirname(filepath)
            best_path = os.path.join(dir_name, "best_model_50.pth")
            # ------------------------------------------
            torch.save(checkpoint, best_path)
            print(f"Best model saved: {best_path}")

def train_icu_model(args, train_dataloader, val_dataloader):
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    print("\n[æ­¥éª¤ 2] åˆ›å»ºæ¨¡å‹ (Longformer åˆå§‹ä¸ºå†»ç»“çŠ¶æ€)...")
    clinical_bert_model = AutoModel.from_pretrained(args.bert_model_name)
    model = create_icu_model(clinical_bert_model, freeze_backbone=(args.fine_tune_epochs == 0))
    
    trainer = None
    best_val_metric_sum, best_f1, best_auprc, best_epoch = 0, 0, 0, 0
    patience, patience_counter = 8, 0
    
    print(f"\n[æ­¥éª¤ 5] å¼€å§‹è®­ç»ƒï¼Œå…± {args.epochs} ä¸ª epoch...")
    
    for epoch in range(args.epochs):
        print(f"\nEpoch {epoch+1}/{args.epochs}")
        start_time = time.time()
        
        is_fine_tuning_epoch = epoch < args.fine_tune_epochs

        if is_fine_tuning_epoch:
            if epoch == 0: print(f"** [ç­–ç•¥] å‰ {args.fine_tune_epochs} ä¸ª Epoch å°†å¾®è°ƒ Longformer **")
            for param in model.text_encoder.bert.parameters():
                param.requires_grad = True
            optimizer_grouped_parameters = [
                {"params": model.text_encoder.bert.parameters(), "lr": args.backbone_lr},
                {"params": [p for n, p in model.named_parameters() if 'text_encoder.bert' not in n and p.requires_grad], "lr": args.learning_rate}
            ]
            optimizer = AdamW(optimizer_grouped_parameters, weight_decay=1e-4)
        else:
            if epoch == args.fine_tune_epochs:
                print(f"** [ç­–ç•¥] ä»æœ¬ Epoch ({epoch+1}) å¼€å§‹ï¼Œå†»ç»“ Longformer **")
            for param in model.text_encoder.bert.parameters():
                param.requires_grad = False
            params_to_optimize = [p for p in model.parameters() if p.requires_grad]
            optimizer = AdamW(params_to_optimize, lr=args.learning_rate, weight_decay=1e-4)

        if trainer is None or is_fine_tuning_epoch != (epoch-1 < args.fine_tune_epochs):
            print("  > ä¼˜åŒ–ç­–ç•¥æ”¹å˜ï¼Œé‡æ–°åˆå§‹åŒ– Trainer...")
            num_training_steps = args.epochs * len(train_dataloader)
            num_warmup_steps = int(0.1 * num_training_steps)
            scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)
            trainer = ICUTrainer(model, device, optimizer, scheduler, class_weights=args.class_weights, use_amp=args.use_amp)
        else:
             trainer.optimizer = optimizer
             trainer.scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=(args.epochs-epoch)*len(train_dataloader))
        
        # --- æ ¸å¿ƒä¿®æ”¹ 3: å°†æ¨¡æ€ä¸¢å¼ƒç‡ä¼ é€’ç»™ trainer ---
        train_loss = trainer.train_epoch(
            train_dataloader, 
            gradient_accumulation_steps=args.grad_accum_steps,
            modality_dropout_rate=args.modality_dropout_rate
        )
        
        val_f1, val_auprc = trainer.validate(val_dataloader)
        
        print(f"è®­ç»ƒ - å¹³å‡æŸå¤±: {train_loss:.4f}")
        print(f"éªŒè¯ - F1åˆ†æ•°: {val_f1:.4f}, AUPRC: {val_auprc:.4f}")
            
        current_metric_sum = val_f1 + val_auprc
        if current_metric_sum > best_val_metric_sum:
            best_val_metric_sum = current_metric_sum
            best_f1, best_auprc, best_epoch = val_f1, val_auprc, epoch + 1
            patience_counter = 0
            print(f"ğŸ‰ æ–°çš„æœ€ä½³æŒ‡æ ‡: F1+AUPRC = {best_val_metric_sum:.4f}")
            is_best = True
        else:
            patience_counter += 1
            print(f"éªŒè¯é›†æŒ‡æ ‡æ— æå‡ ({patience_counter}/{patience})")
            is_best = False
            
        checkpoint_path = os.path.join(args.save_dir, f"icu_model_epoch_{epoch+1}.pth")
        trainer.save_checkpoint(checkpoint_path, epoch + 1, is_best=is_best)
            
        if patience_counter >= patience:
            print(f"æ—©åœï¼šæŒ‡æ ‡è¿ç»­ {patience} ä¸ªepochæ— æå‡ã€‚")
            break
        
        print(f"Epochè€—æ—¶: {time.time() - start_time:.2f}ç§’")

    print("\n" + "="*60 + "\nè®­ç»ƒå®Œæˆæ€»ç»“:")
    if best_epoch > 0:
        print(f"æœ€ä½³éªŒè¯é›†æŒ‡æ ‡ (åœ¨ Epoch {best_epoch}): F1 Score: {best_f1:.4f}, AUPRC: {best_auprc:.4f}")
        best_model_path = os.path.join(args.save_dir, 'icu_model_best_drop.pth')
        print(f"æœ€ä½³æ¨¡å‹æƒé‡å·²ä¿å­˜è‡³: {best_model_path}")
    else:
        print("æœªèƒ½åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ‰¾åˆ°æ›´ä¼˜çš„æ¨¡å‹ã€‚")
    print("=" * 60)
    return model

if __name__ == "__main__":
    # --- æ ¸å¿ƒå¼€å…³ ---
    USE_SUBSET_MODE = False

    # --- å‚æ•°é…ç½® ---
    class TrainingArgs:
        def __init__(self):
            self.data_path = "Data/ihm"
            self.bert_model_name = "yikuan8/Clinical-Longformer"
            self.save_dir = "my_model_weights_finetuned"
            self.epochs = 25
            self.learning_rate = 5e-5
            self.backbone_lr = 2e-5
            self.fine_tune_epochs = 3
            self.use_amp = True
            self.class_weights = torch.FloatTensor([0.57515432, 3.82648871])
            self.seed = 42
            self.effective_batch_size = 32
            self.physical_batch_size = 2 if not USE_SUBSET_MODE else 4
            self.grad_accum_steps = self.effective_batch_size // self.physical_batch_size
            
            # --- æ ¸å¿ƒä¿®æ”¹ 4: åœ¨è¿™é‡Œè®¾ç½®æ¨¡æ€ä¸¢å¼ƒç‡ ---
            # è®ºæ–‡å»ºè®®å€¼ä¸º 0.7 (70%)
            self.modality_dropout_rate = 0.7 if not USE_SUBSET_MODE else 0.2
    
    args = TrainingArgs()
    torch.manual_seed(args.seed); np.random.seed(args.seed)
    
    print(f"--- æ¨¡å¼: {'å¿«é€Ÿè°ƒè¯• (å­é›†)' if USE_SUBSET_MODE else 'æ­£å¼è®­ç»ƒ (å…¨é‡)'} ---")
    print(f"å°†ä½¿ç”¨æœ‰æ•ˆæ‰¹æ¬¡: {args.effective_batch_size} (ç‰©ç†æ‰¹æ¬¡: {args.physical_batch_size}, ç´¯ç§¯æ­¥æ•°: {args.grad_accum_steps})")
    if args.modality_dropout_rate > 0:
        print(f"** æ¨¡æ€ä¸¢å¼ƒå·²å¯ç”¨ï¼Œä¸¢å¼ƒç‡: {args.modality_dropout_rate:.1f} **")
    
    # ... (åç»­çš„æ•°æ®åŠ è½½å’Œæµ‹è¯•é€»è¾‘ä¿æŒä¸å˜) ...
    class DataArgs:
        def __init__(self):
            self.file_path = args.data_path
            self.train_batch_size, self.eval_batch_size = args.physical_batch_size, args.physical_batch_size * 2
            self.debug = False; self.max_length = 1024; self.num_of_notes = 5
            self.tt_max = 48; self.pad_to_max_length = True; self.notes_order = "Last"
            self.modeltype = "TS_Text"; self.model_name = args.bert_model_name
            self.chunk = False; self.ratio_notes_order = None

    data_args = DataArgs()
    tokenizer = AutoTokenizer.from_pretrained(args.bert_model_name)
    
    print("--- æ­£åœ¨åŠ è½½å®Œæ•´æ•°æ®é›† ---")
    full_train_dataset, _, _ = data_perpare(data_args, 'train', tokenizer)
    full_val_dataset, _, _ = data_perpare(data_args, 'val', tokenizer)
    full_test_dataset, _, _ = data_perpare(data_args, 'test', tokenizer)
    
    if USE_SUBSET_MODE:
        train_size = int(0.2 * len(full_train_dataset))
        train_indices = np.random.choice(len(full_train_dataset), train_size, replace=False)
        train_dataset = Subset(full_train_dataset, train_indices)
        print(f"ä½¿ç”¨è®­ç»ƒå­é›†: {len(train_dataset)} / {len(full_train_dataset)} ä¸ªæ ·æœ¬")

        val_size = int(0.5 * len(full_val_dataset))
        val_indices = np.random.choice(len(full_val_dataset), val_size, replace=False)
        val_dataset = Subset(full_val_dataset, val_indices)
        print(f"ä½¿ç”¨éªŒè¯å­é›†: {len(val_dataset)} / {len(full_val_dataset)} ä¸ªæ ·æœ¬")
    else:
        train_dataset, val_dataset = full_train_dataset, full_val_dataset
        print(f"ä½¿ç”¨å…¨éƒ¨è®­ç»ƒæ•°æ®: {len(train_dataset)} ä¸ªæ ·æœ¬")
        print(f"ä½¿ç”¨å…¨éƒ¨éªŒè¯æ•°æ®: {len(val_dataset)} ä¸ªæ ·æœ¬")

    train_dataloader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=data_args.train_batch_size, collate_fn=TextTSIrgcollate_fn)
    val_dataloader = DataLoader(val_dataset, sampler=SequentialSampler(val_dataset), batch_size=data_args.eval_batch_size, collate_fn=TextTSIrgcollate_fn)
    
    trained_model = train_icu_model(args, train_dataloader, val_dataloader)
    
    print("\n--- å¼€å§‹æœ€ç»ˆæµ‹è¯• ---")
    if USE_SUBSET_MODE:
        test_size = int(0.5 * len(full_test_dataset))
        test_indices = np.random.choice(len(full_test_dataset), test_size, replace=False)
        test_dataset = Subset(full_test_dataset, test_indices)
        print(f"ä½¿ç”¨æµ‹è¯•å­é›†: {len(test_dataset)} / {len(full_test_dataset)} ä¸ªæ ·æœ¬")
    else:
        test_dataset = full_test_dataset
        print(f"ä½¿ç”¨å…¨éƒ¨æµ‹è¯•æ•°æ®: {len(test_dataset)} ä¸ªæ ·æœ¬")
        
    test_dataloader = DataLoader(test_dataset, sampler=SequentialSampler(test_dataset), batch_size=data_args.eval_batch_size, collate_fn=TextTSIrgcollate_fn)

    trained_model.to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))
    test_f1, test_auprc = ICUTrainer(trained_model, torch.device('cuda' if torch.cuda.is_available() else 'cpu'), None, None).validate(test_dataloader)
    print("\n--- æµ‹è¯•é›†æœ€ç»ˆç»“æœ ---")
    print(f"  - F1 Score: {test_f1:.4f}")
    print(f"  - AUPRC: {test_auprc:.4f}")