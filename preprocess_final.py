import os
import pickle
from tqdm import tqdm
import torch
from transformers import AutoTokenizer
# ä»ä½ åŸå§‹çš„ data.py ä¸­å¯¼å…¥éœ€è¦çš„å‡½æ•°
from data import F_impute, load_data as load_original_data
import numpy as np

def preprocess_and_save(original_data_path, output_dir, mode, tokenizer, max_len=1024, tt_max=48):
    """
    æ‰§è¡Œæœ€ç»ˆçš„é¢„å¤„ç†æ­¥éª¤ (impute å’Œ tokenize) å¹¶å°†ç»“æœä¿å­˜åˆ°æ–°ç›®å½•ã€‚
    """
    print(f"--- å¼€å§‹å¤„ç† '{mode}' æ•°æ® ---")
    
    # 1. ä»åŸå§‹è·¯å¾„åŠ è½½åŠæˆå“æ•°æ®
    try:
        # ä½¿ç”¨åŸå§‹çš„ load_data å‡½æ•°
        original_data = load_original_data(file_path=original_data_path, mode=mode, debug=False)
    except FileNotFoundError as e:
        print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ° '{mode}' æ¨¡å¼çš„åŸå§‹æ•°æ®ï¼Œå·²è·³è¿‡ã€‚è¯·æ£€æŸ¥è·¯å¾„ '{original_data_path}'ã€‚")
        print(f"   è¯¦ç»†é”™è¯¯: {e}")
        return

    processed_data = []
    for item in tqdm(original_data, desc=f"æ­£åœ¨å¤„ç† {mode} æ•°æ®"):
        
        # è·³è¿‡å¯èƒ½ä¸å®Œæ•´çš„æ•°æ®é¡¹
        if 'irg_ts' not in item or 'text_data' not in item:
            print(f"è­¦å‘Šï¼šè·³è¿‡ä¸€ä¸ªä¸å®Œæ•´çš„æ•°æ®é¡¹ã€‚")
            continue

        # 2. æ‰§è¡Œ F_impute è®¡ç®—
        reg_ts = F_impute(item['irg_ts'], item['ts_tt'], item['irg_ts_mask'], 1, tt_max)
        
        # 3. æ‰§è¡Œ Tokenization
        text_token = []
        atten_mask = []
        for t in item['text_data']:
            inputs = tokenizer.encode_plus(
                t,
                padding="max_length",
                max_length=max_len,
                add_special_tokens=True,
                return_attention_mask=True,
                truncation=True
            )
            text_token.append(inputs['input_ids'])
            
            attention_mask = inputs['attention_mask']
            # å…¼å®¹ Longformer çš„ç‰¹æ®Š attention mask å¤„ç†
            if "Longformer" in tokenizer.name_or_path:
                attention_mask[0] += 1
            atten_mask.append(attention_mask)

        # 4. ç»„è£…æ‰€æœ‰å¤„ç†å¥½çš„æ•°æ®åˆ°ä¸€ä¸ªæ–°çš„å­—å…¸ä¸­
        new_item = item.copy()
        # å­˜å‚¨ä¸º numpy array ä»¥ä¾¿åç»­å¿«é€ŸåŠ è½½
        new_item['reg_ts_imputed'] = np.array(reg_ts, dtype=np.float32)
        new_item['input_ids_tokenized'] = [np.array(t, dtype=np.int32) for t in text_token]
        new_item['attention_mask_tokenized'] = [np.array(a, dtype=np.int32) for a in atten_mask]
        
        processed_data.append(new_item)

    # 5. ä¿å­˜åˆ°æ–°çš„ Data_Final æ–‡ä»¶å¤¹
    os.makedirs(output_dir, exist_ok=True)
    output_path = os.path.join(output_dir, f"{mode}_final_data.pkl")
    
    with open(output_path, 'wb') as f:
        pickle.dump(processed_data, f)
        
    print(f"âœ… '{mode}' æ•°æ®å¤„ç†å®Œæˆï¼Œå·²ä¿å­˜è‡³: {output_path}\n")

if __name__ == "__main__":
    # --- é…ç½® ---
    ORIGINAL_DATA_PATH = "Data/ihm"
    FINAL_DATA_PATH = "Data_Final"
    BERT_MODEL_NAME = "yikuan8/Clinical-Longformer"
    
    print("æ­£åœ¨åŠ è½½ Tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL_NAME)
    
    # --- å¯¹æ‰€æœ‰æ•°æ®æ¨¡å¼æ‰§è¡Œé¢„å¤„ç† ---
    preprocess_and_save(ORIGINAL_DATA_PATH, FINAL_DATA_PATH, 'train', tokenizer)
    preprocess_and_save(ORIGINAL_DATA_PATH, FINAL_DATA_PATH, 'val', tokenizer)
    preprocess_and_save(ORIGINAL_DATA_PATH, FINAL_DATA_PATH, 'test', tokenizer)
    
    print("ğŸ‰ æ‰€æœ‰æ•°æ®é¢„å¤„ç†å®Œæˆï¼ç°åœ¨ä½ å¯ä»¥ä½¿ç”¨ data_final.py æ¥è¿›è¡Œè®­ç»ƒäº†ã€‚")
