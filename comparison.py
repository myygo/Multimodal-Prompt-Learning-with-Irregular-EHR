#!/usr/bin/env python3
"""
ä½¿ç”¨çœŸå®æ•°æ®å¯¹ ICUPromptModel å’Œæ‰€æœ‰ baseline æ¨¡å‹è¿›è¡Œç»¼åˆæ€§èƒ½æµ‹è¯•ã€‚
æ”¯æŒå®Œæ•´çš„ç¼ºå¤±æ¨¡æ€æƒ…æ™¯å’Œæ¨¡æ€å†…éƒ¨ç¼ºå¤±æƒ…æ™¯çš„æµ‹è¯•ã€‚
"""

import torch
import torch.nn as nn
import numpy as np
import pandas as pd
import pickle
import os
from sklearn.metrics import accuracy_score, f1_score, average_precision_score
from torch.utils.data import DataLoader, Dataset
import warnings
import random

# åŠ¨æ€å¯¼å…¥
from data import data_perpare
from my_model import create_icu_model # å¯¼å…¥ä½ çš„æ¨¡å‹
from baseline_methods import create_adapted_baselines # å¯¼å…¥ baseline

# å°è¯•å¯¼å…¥ transformers
try:
    from transformers import AutoTokenizer
    TRANSFORMERS_AVAILABLE = True
    print("âœ… Transformers åº“åŠ è½½æˆåŠŸ")
except ImportError:
    TRANSFORMERS_AVAILABLE = False

warnings.filterwarnings('ignore')

# --- NEW: å®ç°æ¨¡æ€å†…éƒ¨ç¼ºå¤±çš„å‡½æ•° ---
def apply_intra_modal_missing(batch_data, missing_rate, random_seed=42):
    """
    å¯¹ä¸€ä¸ªæ•°æ®æ‰¹æ¬¡åº”ç”¨æ¨¡æ€å†…éƒ¨çš„éšæœºç¼ºå¤±ã€‚
    è¿™ä¼šéšæœºé®è”½æ‰ä¸€éƒ¨åˆ†æ—¶åºæ•°æ®ç‚¹å’Œæ–‡æœ¬è¯å…ƒã€‚
    """
    if random_seed is not None:
        torch.manual_seed(random_seed)
        np.random.seed(random_seed)
        random.seed(random_seed)
    
    (ts_input_sequences, ts_mask_sequences, ts_tt, reg_ts_input,
     input_ids, attn_mask, note_time, note_time_mask, label) = batch_data
    
    # 1. å¯¹æ–‡æœ¬æ•°æ®åº”ç”¨ç¼ºå¤± (é€šè¿‡ä¿®æ”¹ attention mask)
    if attn_mask is not None:
        new_attn_mask = attn_mask.clone()
        for b in range(attn_mask.shape[0]):
            for n in range(attn_mask.shape[1]):
                valid_positions = (attn_mask[b, n] == 1).nonzero(as_tuple=False).squeeze(-1)
                if len(valid_positions) > 0:
                    num_to_mask = int(len(valid_positions) * missing_rate)
                    if num_to_mask > 0:
                        mask_indices = torch.randperm(len(valid_positions))[:num_to_mask]
                        positions_to_mask = valid_positions[mask_indices]
                        new_attn_mask[b, n, positions_to_mask] = 0
        attn_mask = new_attn_mask
    
    # 2. å¯¹è§„åˆ™æ—¶åºæ•°æ®åº”ç”¨ç¼ºå¤± (ç›´æ¥å°†å€¼è®¾ä¸º0)
    if reg_ts_input is not None:
        new_reg_ts = reg_ts_input.clone()
        batch_size, seq_len, num_features = new_reg_ts.shape
        
        num_steps_to_mask = int(seq_len * missing_rate)
        if num_steps_to_mask > 0:
            for b in range(batch_size):
                mask_indices = torch.randperm(seq_len)[:num_steps_to_mask]
                new_reg_ts[b, mask_indices, :] = 0
        reg_ts_input = new_reg_ts

    return (ts_input_sequences, ts_mask_sequences, ts_tt, reg_ts_input,
            input_ids, attn_mask, note_time, note_time_mask, label)


# --- MODIFIED: æ›´æ–°æµ‹è¯•æƒ…æ™¯ ---
def create_test_scenarios():
    """åˆ›å»ºä¸å›¾ç‰‡åŒ¹é…çš„æµ‹è¯•æƒ…æ™¯"""
    return {
        'Complete_Data': {'text_missing': False, 'numerical_missing': False, 'intra_missing': False, 'missing_rate': 0.0, 'description': 'å®Œæ•´æ•°æ®'},
        'Missing_Text': {'text_missing': True, 'numerical_missing': False, 'intra_missing': False, 'missing_rate': 0.0, 'description': 'ç¼ºå¤±æ–‡æœ¬ (ä»…æ—¶åº)'},
        'Missing_Numerical': {'text_missing': False, 'numerical_missing': True, 'intra_missing': False, 'missing_rate': 0.0, 'description': 'ç¼ºå¤±æ—¶åº (ä»…æ–‡æœ¬)'},
        'Intra_Missing_20': {'text_missing': False, 'numerical_missing': False, 'intra_missing': True, 'missing_rate': 0.2, 'description': 'å†…éƒ¨ç¼ºå¤±20%'},
        'Intra_Missing_40': {'text_missing': False, 'numerical_missing': False, 'intra_missing': True, 'missing_rate': 0.4, 'description': 'å†…éƒ¨ç¼ºå¤±40%'},
        'Intra_Missing_60': {'text_missing': False, 'numerical_missing': False, 'intra_missing': True, 'missing_rate': 0.6, 'description': 'å†…éƒ¨ç¼ºå¤±60%'},
        'Intra_Missing_80': {'text_missing': False, 'numerical_missing': False, 'intra_missing': True, 'missing_rate': 0.8, 'description': 'å†…éƒ¨ç¼ºå¤±80%'},
    }

# --- è¾…åŠ©å‡½æ•°ï¼šåŠ è½½æ¨¡å‹ ---
def load_my_trained_model(weight_path, bert_model): # <--- ä¿®æ”¹ï¼šæ¥æ”¶ bert_model
    print(f"æ­£åœ¨åŠ è½½ä½ çš„æ¨¡å‹æƒé‡: {weight_path}")
    if not os.path.exists(weight_path):
        print(f"âŒ è­¦å‘Š: æ‰¾ä¸åˆ°æƒé‡æ–‡ä»¶ '{weight_path}'ã€‚")
        return None
    try:
        model = create_icu_model(bert_model) # <--- ä¿®æ”¹ï¼šå°† bert_model ä¼ è¿›å»
        checkpoint = torch.load(weight_path, map_location='cpu')
        model.load_state_dict(checkpoint['model_state_dict'])
        model.eval()
        print("âœ… ä½ çš„æ¨¡å‹åŠ è½½æˆåŠŸ")
        return model
    except Exception as e:
        print(f"âŒ åŠ è½½ä½ çš„æ¨¡å‹æƒé‡æ—¶å‡ºé”™: {e}ã€‚")
        return None

# --- FIXED: å°† Args ç±»å®šä¹‰ç§»åˆ°å‡½æ•°å¤–éƒ¨ ---
class Args:
    def __init__(self, file_path, batch_size, debug, task):
        self.file_path = file_path
        self.eval_batch_size = batch_size
        self.debug = debug
        self.task = task
        self.max_length = 128
        self.num_of_notes = 3
        self.tt_max = 24
        self.pad_to_max_length = True
        self.notes_order = "Last"
        self.modeltype = "TS_Text"
        self.model_name = "yikuan8/Clinical-Longformer"
        self.chunk = False
        self.ratio_notes_order = None

# --- ä¸»æµ‹è¯•æµç¨‹ ---
def run_comprehensive_comparison(file_path, task='ihm', mode='test', batch_size=8, debug=False):
    """
    è¿è¡Œç»¼åˆæ¨¡å‹å¯¹æ¯”æµ‹è¯•
    """
    torch.manual_seed(42)
    np.random.seed(42)
    random.seed(42)
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {device}")
    
    # --- FIXED: æ­£ç¡®åœ°å®ä¾‹åŒ– Args ç±» ---
    args = Args(file_path, batch_size, debug, task)
    
# --- ä¿®æ”¹å ---
    # å‡†å¤‡æ•°æ®åŠ è½½å™¨å’ŒåŸºç¡€BERTæ¨¡å‹
    if not TRANSFORMERS_AVAILABLE:
        print("âŒ Transformers åº“æœªå®‰è£…ï¼Œæ— æ³•è¿›è¡Œæµ‹è¯•ã€‚")
        return
        
    from transformers import AutoModel # å¯¼å…¥ AutoModel

    tokenizer = AutoTokenizer.from_pretrained(args.model_name)
    print("ğŸ§  æ­£åœ¨åŠ è½½åŸºç¡€BERTæ¨¡å‹...")
    bert_model = AutoModel.from_pretrained(args.model_name) # <--- æ–°å¢ï¼šåŠ è½½åŸºç¡€BERTæ¨¡å‹
    print("âœ… åŸºç¡€BERTæ¨¡å‹åŠ è½½æˆåŠŸ")
    
    _, _, dataloader = data_perpare(args, mode, tokenizer)

    if dataloader is None: return

    # --- åŠ è½½ä½ çš„æ¨¡å‹ ---
    my_model_weight_path = "icu_model_weights_final/icu_model_epoch_13_best.pth" # !!! ç¡®ä¿è·¯å¾„æ­£ç¡®
    my_model = load_my_trained_model(my_model_weight_path,bert_model)
    
    models_to_test = {}
    if my_model:
        models_to_test['ICU_PromptModel'] = my_model
    else:
        print("ä½ çš„æ¨¡å‹æœªèƒ½åŠ è½½ï¼Œæµ‹è¯•æ— æ³•è¿›è¡Œã€‚")
        return

    scenarios = create_test_scenarios()
    results = []

    for model_name, model in models_to_test.items():
        print(f"\n{'='*20} æ­£åœ¨æµ‹è¯•: {model_name} {'='*20}")
        model.to(device).eval()
        
        for scenario_name, config in scenarios.items():
            print(f"  ğŸ”¬ æƒ…æ™¯: {scenario_name} - {config['description']}")
            
            all_labels = []
            all_preds = []
            all_probs = [] # ç”¨äº AUPRC

            for i, batch_data in enumerate(dataloader):
                if batch_data is None: continue

                # åº”ç”¨æ¨¡æ€ç¼ºå¤±æˆ–å†…éƒ¨ç¼ºå¤±
                if config['intra_missing']:
                    batch_data = apply_intra_modal_missing(batch_data, config['missing_rate'], random_seed=42+i)
                
                (ts_input_sequences, ts_mask_sequences, ts_tt, reg_ts_input,
                 input_ids, attn_mask, note_time, note_time_mask, label) = batch_data

                if config['text_missing']:
                    input_ids, attn_mask, note_time, note_time_mask = [None] * 4
                if config['numerical_missing']:
                    ts_input_sequences, ts_mask_sequences, reg_ts_input = [None] * 3

                # å°†æ•°æ®ç§»åŠ¨åˆ°è®¾å¤‡
                batch_on_device = [
                    d.to(device).float() if torch.is_tensor(d) and d.dtype == torch.float32 else
                    d.to(device).long() if torch.is_tensor(d) else None
                    for d in (ts_input_sequences, ts_mask_sequences, ts_tt, reg_ts_input,
                              input_ids, attn_mask, note_time, note_time_mask, label)
                ]
                
                # å‰å‘ä¼ æ’­
                with torch.no_grad():
                    outputs = model(*batch_on_device, intra_missing_ratio=0.0)

                # æ”¶é›†ç»“æœ
                if outputs is not None:
                    probs = torch.softmax(outputs, dim=1)[:, 1].cpu().numpy()
                    preds = torch.argmax(outputs, dim=1).cpu().numpy()
                    all_labels.extend(label.cpu().numpy())
                    all_preds.extend(preds)
                    all_probs.extend(probs)

            # --- MODIFIED: è®¡ç®—æ–°æŒ‡æ ‡ ---
            if all_labels:
                f1 = f1_score(all_labels, all_preds, average='binary' if task == 'ihm' else 'macro')
                micro_f1 = f1_score(all_labels, all_preds, average='micro')
                auprc = average_precision_score(all_labels, all_probs)
                
                results.append({
                    'Model': model_name,
                    'Scenario': scenario_name,
                    'Missing_Rate': f"{config['missing_rate']*100:.0f}%" if config['intra_missing'] else "N/A",
                    'F1-Score': f1,
                    'Micro-F1': micro_f1,
                    'AUPRC': auprc,
                    'Samples': len(all_labels)
                })

    # --- ç»“æœå±•ç¤º ---
    if results:
        df = pd.DataFrame(results)
        print(f"\n{'ğŸ“Š'*10} è¯¦ç»†æµ‹è¯•ç»“æœ {'ğŸ“Š'*10}")
        # æ ¼å¼åŒ–è¾“å‡º
        df_display = df.copy()
        for col in ['F1-Score', 'Micro-F1', 'AUPRC']:
            df_display[col] = df_display[col].map('{:.4f}'.format)
        print(df_display.to_string(index=False))

if __name__ == "__main__":
    DATA_PATH = "data/ihm"
    run_comprehensive_comparison(DATA_PATH, task='ihm', batch_size=16)