#!/usr/bin/env python3
"""
[æœ€ç»ˆå®Œæ•´ç‰ˆ] ç”¨äºè®­ç»ƒ ICUPromptModel çš„è„šæœ¬ã€‚

åŠŸèƒ½äº®ç‚¹:
- æ–°å¢ï¼šå®Œæ•´çš„è‡ªåŠ¨æ··åˆç²¾åº¦è®­ç»ƒ (AMP) æ”¯æŒï¼Œä»¥æå‡é€Ÿåº¦ã€é™ä½æ˜¾å­˜ã€‚
- æ–°å¢ï¼šæ›´è¯¦ç»†çš„è®­ç»ƒä¿¡æ¯è¾“å‡ºï¼ŒåŒ…æ‹¬å¯åŠ¨æ—¶çš„å‚æ•°æ€»è§ˆå’Œå­¦ä¹ ç‡å˜åŒ–æç¤ºã€‚
- ä½¿ç”¨åŠ æƒæŸå¤±å‡½æ•°å¤„ç†ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ã€‚
- æ ¹æ®éªŒè¯é›†ä¸Š F1-Score å’Œ AUPRC çš„æ€»å’Œæ¥é€‰æ‹©å¹¶ä¿å­˜æœ€ä½³æ¨¡å‹ã€‚
- åŒ…å«æ—©åœï¼ˆEarly Stoppingï¼‰æœºåˆ¶ã€‚
- æ”¯æŒâ€œå­é›†è®­ç»ƒâ€æ¨¡å¼ï¼Œç”¨äºå¿«é€Ÿè°ƒè¯•ã€‚
- é›†æˆäº†æ¨¡æ€ä¸¢å¼ƒï¼ˆModality Dropoutï¼‰ä½œä¸ºæ ¸å¿ƒæ­£åˆ™åŒ–ç­–ç•¥ã€‚
"""
from transformers import AutoModel, AutoTokenizer, AdamW, get_linear_schedule_with_warmup
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from sklearn.metrics import accuracy_score, f1_score, average_precision_score
from torch.utils.data import DataLoader
import os
import time
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

# ç¡®ä¿ data.py å’Œ my_model.py åœ¨åŒä¸€ç›®å½•ä¸‹æˆ–Pythonè·¯å¾„ä¸­
from my_model import create_icu_model, ICUHyperParams
from data import data_perpare

class ICUTrainer:
    """å°è£…äº†è®­ç»ƒã€éªŒè¯å’Œæ£€æŸ¥ç‚¹ä¿å­˜çš„é€»è¾‘ï¼Œå¹¶é›†æˆäº†AMP"""
    def __init__(self, model, device, optimizer, scheduler, class_weights=None, use_amp=True):
        self.model = model.to(device)
        self.device = device
        self.optimizer = optimizer
        self.scheduler = scheduler
        self.use_amp = use_amp and torch.cuda.is_available() # åªæœ‰åœ¨CUDAå¯ç”¨æ—¶æ‰å¼€å¯AMP
        
        # æ‰“å°å‚æ•°é‡çš„é€»è¾‘å¯ä»¥ä¿ç•™
        params_to_train = [p for p in self.model.parameters() if p.requires_grad]
        print(f"  > æ¨¡å‹æ€»å‚æ•°é‡: {sum(p.numel() for p in self.model.parameters()):,}")
        print(f"  > å¯è®­ç»ƒå‚æ•°é‡: {sum(p.numel() for p in params_to_train):,}")
        
        if class_weights is not None:
            class_weights = class_weights.to(device)
            print(f"æ­£åœ¨ä½¿ç”¨å¸¦æƒé‡çš„æŸå¤±å‡½æ•°ï¼Œæƒé‡ä¸º: {class_weights.cpu().numpy()}")
        self.criterion = nn.CrossEntropyLoss(weight=class_weights)
        
        if self.use_amp:
            self.scaler = torch.cuda.amp.GradScaler()
            print("  > è‡ªåŠ¨æ··åˆç²¾åº¦è®­ç»ƒ (AMP) å·²å¯ç”¨ã€‚")

    def train_epoch(self, dataloader, use_subset=False, modality_missing_rate=0.0):
        self.model.train()
        total_loss = 0
        all_preds, all_labels = [], []
        
        num_batches_to_use = len(dataloader)
        if use_subset:
            num_batches_to_use = int(len(dataloader) * 0.1)
            print(f"  [å­é›†è®­ç»ƒæ¨¡å¼] æ¯ä¸ªEpochåªä½¿ç”¨ {num_batches_to_use} / {len(dataloader)} ä¸ªè®­ç»ƒæ‰¹æ¬¡ (10%)")

        update_interval = max(1, len(dataloader) // 4)
        progress_bar = tqdm(
            dataloader, 
            desc="Training", 
            miniters=update_interval,
            mininterval=float('inf')
        )
        
        for batch_idx, batch_data in enumerate(progress_bar):
            if use_subset and batch_idx >= num_batches_to_use:
                break
            if batch_data is None: continue
            
            try:
                (ts_input_sequences, ts_mask_sequences, ts_tt, reg_ts_input,
                 input_ids, attn_mask, note_time, note_time_mask, label) = [d.to(self.device) if torch.is_tensor(d) else None for d in batch_data]

                if self.model.training and torch.rand(1).item() < modality_missing_rate:
                    if torch.rand(1).item() < 0.5:
                        input_ids, attn_mask, note_time, note_time_mask = None, None, None, None
                    else:
                        ts_input_sequences, ts_mask_sequences, ts_tt, reg_ts_input = None, None, None, None
                
                self.optimizer.zero_grad(set_to_none=True) # ä½¿ç”¨ set_to_none=True ç•¥å¾®æå‡æ€§èƒ½

                # ä½¿ç”¨ AMP
                with torch.cuda.amp.autocast(enabled=self.use_amp):
                    outputs = self.model(
                        ts_input_sequences=ts_input_sequences, ts_mask_sequences=ts_mask_sequences,
                        ts_tt=ts_tt, reg_ts_input=reg_ts_input, input_ids=input_ids,
                        attn_mask=attn_mask, note_time=note_time, note_time_mask=note_time_mask,
                        label=label, intra_missing_ratio=0.1
                    )
                    if outputs is None: continue
                    loss = self.criterion(outputs, label)

                if self.use_amp:
                    self.scaler.scale(loss).backward()
                    self.scaler.step(self.optimizer)
                    self.scaler.update()
                else:
                    loss.backward()
                    self.optimizer.step()
                
                self.scheduler.step() # åœ¨æ¯ä¸ªstepåæ›´æ–°å­¦ä¹ ç‡

                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                
                total_loss += loss.item()
                _, predicted = torch.max(outputs, 1)
                all_preds.extend(predicted.cpu().numpy())
                all_labels.extend(label.cpu().numpy())
                progress_bar.set_postfix({'Loss': f'{loss.item():.4f}'}, refresh=False)
                
            except Exception as e:
                print(f"\nè®­ç»ƒæ‰¹æ¬¡ {batch_idx} å‘ç”Ÿé”™è¯¯: {e}")
                import traceback
                traceback.print_exc()
                continue
        
        avg_loss = total_loss / len(all_labels) if all_labels else 0
        accuracy = accuracy_score(all_labels, all_preds) if all_labels else 0
        return avg_loss, accuracy

    def validate(self, dataloader, use_subset=False):
        self.model.eval()
        all_preds, all_labels, all_probs = [], [], []
        
        num_batches_to_use = len(dataloader)
        if use_subset:
            num_batches_to_use = int(len(dataloader) * (1/3))
            print(f"  [å­é›†éªŒè¯æ¨¡å¼] æ¯ä¸ªEpochåªä½¿ç”¨ {num_batches_to_use} / {len(dataloader)} ä¸ªéªŒè¯æ‰¹æ¬¡ (1/3)")
        
        with torch.no_grad():
            update_interval = max(1, len(dataloader) // 4)
            progress_bar = tqdm(dataloader, desc="Validating", miniters=update_interval, mininterval=float('inf'))
            for batch_idx, batch_data in enumerate(progress_bar):
                if use_subset and batch_idx >= num_batches_to_use:
                    break
                if batch_data is None: continue
                
                try:
                    (ts_input_sequences, ts_mask_sequences, ts_tt, reg_ts_input,
                     input_ids, attn_mask, note_time, note_time_mask, label) = [d.to(self.device) if torch.is_tensor(d) else None for d in batch_data]
                    
                    with torch.cuda.amp.autocast(enabled=self.use_amp):
                        outputs = self.model(
                            ts_input_sequences=ts_input_sequences, ts_mask_sequences=ts_mask_sequences,
                            ts_tt=ts_tt, reg_ts_input=reg_ts_input, input_ids=input_ids,
                            attn_mask=attn_mask, note_time=note_time, note_time_mask=note_time_mask,
                            label=label, intra_missing_ratio=0.0
                        )
                    
                    if outputs is None: continue
                    probs = torch.softmax(outputs, dim=1)[:, 1]
                    predicted = torch.argmax(outputs, 1)
                    all_preds.extend(predicted.cpu().numpy())
                    all_labels.extend(label.cpu().numpy())
                    all_probs.extend(probs.cpu().numpy())

                except Exception as e:
                    print(f"\néªŒè¯æ‰¹æ¬¡å‘ç”Ÿé”™è¯¯: {e}")
                    continue
        
        if all_labels:
            accuracy = accuracy_score(all_labels, all_preds)
            f1 = f1_score(all_labels, all_preds, average='binary')
            auprc = average_precision_score(all_labels, all_probs)
            return accuracy, f1, auprc
        return 0.0, 0.0, 0.0

    def save_checkpoint(self, filepath, epoch, is_best=False):
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        checkpoint = {'epoch': epoch, 'model_state_dict': self.model.state_dict()}
        torch.save(checkpoint, filepath)
        # print(f"Checkpoint saved: {filepath}")
        if is_best:
            best_path = filepath.replace('.pth', '_best.pth')
            torch.save(checkpoint, best_path)
            print(f"Best model saved: {best_path}")

class ArgsPlaceholder:
    def __init__(self, data_path, batch_size, bert_model_name):
        self.file_path = data_path
        self.train_batch_size, self.eval_batch_size = batch_size, batch_size
        self.debug = False
        self.max_length, self.num_of_notes, self.tt_max = 512, 5, 48
        self.pad_to_max_length, self.notes_order = True, "Last"
        self.modeltype, self.model_name = "TS_Text", bert_model_name
        self.chunk, self.ratio_notes_order = False, None

def train_icu_model(data_path, epochs, batch_size, learning_rate, modality_dropout_rate, save_dir, bert_model_name, use_subset, use_amp, freeze_backbone):
    print("å¼€å§‹è®­ç»ƒ ICUPromptModel")
    print("=" * 60)
    
    print("è®­ç»ƒå‚æ•°æ€»è§ˆ:")
    print(f"  - æ•°æ®è·¯å¾„: {data_path}")
    print(f"  - BERTæ¨¡å‹: {bert_model_name}")
    print(f"  - ä¿å­˜ç›®å½•: {save_dir}")
    print(f"  - è®­ç»ƒè½®æ•° (Epochs): {epochs}")
    print(f"  - æ‰¹å¤„ç†å¤§å° (Batch Size): {batch_size}")
    print(f"  - å­¦ä¹ ç‡ (Learning Rate): {learning_rate}")
    print(f"  - æ¨¡æ€ä¸¢å¼ƒç‡: {modality_dropout_rate}")
    print(f"  - ä½¿ç”¨æ··åˆç²¾åº¦ (AMP): {use_amp}")
    print(f"  - å†»ç»“BERTéª¨å¹²: {freeze_backbone}")
    print(f"  - ä½¿ç”¨æ•°æ®å­é›†: {use_subset}")
    print("-" * 60)

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    torch.manual_seed(42); np.random.seed(42)
    
    print("\n[æ­¥éª¤ 1] å‡†å¤‡æ•°æ®åŠ è½½å™¨...")
    tokenizer = AutoTokenizer.from_pretrained(bert_model_name)
    args = ArgsPlaceholder(data_path, batch_size, bert_model_name)
    train_dataset, _, train_dataloader = data_perpare(args, 'train', tokenizer)
    _, _, val_dataloader = data_perpare(args, 'val', tokenizer)
    
    fixed_weights = [0.57515432, 3.82648871]
    class_weights = torch.FloatTensor(fixed_weights)
    
    print("\n[æ­¥éª¤ 2] åˆ›å»ºæ¨¡å‹...")
    print(f"æ­£åœ¨ä»Hugging FaceåŠ è½½é¢„è®­ç»ƒæ¨¡å‹: {bert_model_name}")
    clinical_bert_model = AutoModel.from_pretrained(bert_model_name)
    model = create_icu_model(clinical_bert_model, freeze_backbone=freeze_backbone)
    
    print("\n[æ­¥éª¤ 3] åˆå§‹åŒ–è®­ç»ƒå™¨...")

    backbone_lr = 2e-5  # é¢„è®­ç»ƒä¸»å¹²çš„ä½å­¦ä¹ ç‡
    head_lr = learning_rate # è‡ªå®šä¹‰æ¨¡å—ä½¿ç”¨æ‚¨è®¾ç½®çš„è¾ƒé«˜å­¦ä¹ ç‡

    print(f"  > å·®åˆ†å­¦ä¹ ç‡: Backbone LR = {backbone_lr}, Fusion Head LR = {head_lr}")

    optimizer_grouped_parameters = [
        {
            "params": [p for n, p in model.named_parameters() if "text_encoder" in n],
            "lr": backbone_lr,
        },
        {
            "params": [p for n, p in model.named_parameters() if "text_encoder" not in n],
            "lr": head_lr,
            "weight_decay": 1e-4
        },
    ]
    optimizer = AdamW(optimizer_grouped_parameters)

    num_training_steps = epochs * len(train_dataloader)
    num_warmup_steps = int(0.1 * num_training_steps) # ä½¿ç”¨10%çš„æ­¥æ•°è¿›è¡Œé¢„çƒ­
    print(f"  > æ€»è®­ç»ƒæ­¥æ•°: {num_training_steps}, é¢„çƒ­æ­¥æ•°: {num_warmup_steps}")

    scheduler = get_linear_schedule_with_warmup(
        optimizer,
        num_warmup_steps=num_warmup_steps,
        num_training_steps=num_training_steps
    )
    num_training_steps = epochs * len(train_dataloader)
    num_warmup_steps = int(0.1 * num_training_steps) # ä½¿ç”¨10%çš„æ­¥æ•°è¿›è¡Œé¢„çƒ­
    print(f"  > æ€»è®­ç»ƒæ­¥æ•°: {num_training_steps}, é¢„çƒ­æ­¥æ•°: {num_warmup_steps}")

    scheduler = get_linear_schedule_with_warmup(
        optimizer,
        num_warmup_steps=num_warmup_steps,
        num_training_steps=num_training_steps
    )
    # --- [ä¿®æ”¹ç»“æŸ] ---


    print("\n[æ­¥éª¤ 3.5] åˆå§‹åŒ–è®­ç»ƒå™¨...")
    # --- [ä¿®æ”¹ä»£ç ï¼šä¼ å…¥æ–°çš„optimizerå’Œscheduler] ---

    trainer = ICUTrainer(model, device, optimizer, scheduler, class_weights=class_weights, use_amp=use_amp)
    
    best_val_metric_sum, best_f1, best_auprc, best_epoch = 0, 0, 0, 0
    patience, patience_counter = 8, 0
    
    print(f"\n[æ­¥éª¤ 4] å¼€å§‹è®­ç»ƒï¼Œå…± {epochs} ä¸ª epoch...")
    
    for epoch in range(epochs):
        print(f"\nEpoch {epoch+1}/{epochs}")
        start_time = time.time()
        
        train_loss, train_acc = trainer.train_epoch(train_dataloader, use_subset=use_subset, modality_missing_rate=modality_dropout_rate)
        
        if val_dataloader:
            val_acc, val_f1, val_auprc = trainer.validate(val_dataloader, use_subset=use_subset)
            print(f"è®­ç»ƒ - æŸå¤±: {train_loss:.4f}, å‡†ç¡®ç‡: {train_acc:.4f}")
            print(f"éªŒè¯ - å‡†ç¡®ç‡: {val_acc:.4f}, F1åˆ†æ•°: {val_f1:.4f}, AUPRC: {val_auprc:.4f}")
            
            trainer.scheduler.step(val_f1 + val_auprc)
            current_metric_sum = val_f1 + val_auprc
            is_best = current_metric_sum > best_val_metric_sum
            
            if is_best:
                best_val_metric_sum = current_metric_sum
                best_f1, best_auprc, best_epoch = val_f1, val_auprc, epoch + 1
                patience_counter = 0
                print(f"ğŸ‰ æ–°çš„æœ€ä½³æŒ‡æ ‡: F1+AUPRC = {best_val_metric_sum:.4f} (F1: {best_f1:.4f}, AUPRC: {best_auprc:.4f})")
            else:
                patience_counter += 1
                print(f"éªŒè¯é›†æŒ‡æ ‡æ— æå‡ ({patience_counter}/{patience})")
            
            checkpoint_path = os.path.join(save_dir, f"icu_model_epoch_{epoch+1}.pth")
            trainer.save_checkpoint(checkpoint_path, epoch + 1, is_best=is_best)
            
            if patience_counter >= patience:
                print(f"æ—©åœï¼šæŒ‡æ ‡è¿ç»­ {patience} ä¸ªepochæ— æå‡ã€‚")
                break
        
        print(f"Epochè€—æ—¶: {time.time() - start_time:.2f}ç§’")

    print(f"\n{'='*60}")
    print("è®­ç»ƒå®Œæˆæ€»ç»“:")
    if best_epoch > 0:
        print(f"æœ€ä½³éªŒè¯é›†æŒ‡æ ‡ (åœ¨ Epoch {best_epoch}):")
        print(f"  - F1 Score: {best_f1:.4f}")
        print(f"  - AUPRC: {best_auprc:.4f}")
        print(f"  - F1 + AUPRC: {best_val_metric_sum:.4f}")
        best_model_path = os.path.join(save_dir, f'icu_model_epoch_{best_epoch}_best.pth')
        print(f"æœ€ä½³æ¨¡å‹æƒé‡å·²ä¿å­˜è‡³: {best_model_path}")
    else:
        print("æœªèƒ½åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ‰¾åˆ°æ›´ä¼˜çš„æ¨¡å‹ã€‚")
    print("=" * 60)

if __name__ == "__main__":
    # --- ä¸»è¦å‚æ•°é…ç½®åŒº ---
    # æ‚¨å¯ä»¥åœ¨è¿™é‡Œè½»æ¾åœ°è°ƒæ•´æ‰€æœ‰å…³é”®è¶…å‚æ•°
    
    # æ•°æ®å’Œæ¨¡å‹è·¯å¾„
    DATA_PATH = "data/ihm"
    BERT_MODEL_NAME = "yikuan8/Clinical-Longformer"
    SAVE_DIR = "icu_model_weights_final"
    
    # è®­ç»ƒæ§åˆ¶å‚æ•°
    EPOCHS = 25
    BATCH_SIZE = 16       # 4090æ˜¾å¡ + AMPï¼Œå¯ä»¥å°è¯•æ›´å¤§çš„batch size
    LEARNING_RATE = 4e-4  # å†»ç»“æ¨¡å¼ä¸‹ï¼Œå¯ä»¥ä½¿ç”¨æ›´é«˜çš„å­¦ä¹ ç‡
    MODALITY_DROPOUT_RATE = 0.5
    
    # åŠŸèƒ½å¼€å…³
    USE_AMP = True         # è®¾ç½®ä¸º True æ¥å¼€å¯æ··åˆç²¾åº¦è®­ç»ƒ
    FREEZE_BACKBONE = True # è®¾ç½®ä¸º True æ¥å†»ç»“BERT/Longformerçš„å‚æ•°
    
    # å¿«é€Ÿæµ‹è¯•æ¨¡å¼
    USE_SUBSET_MODE = False
    
    # --- è¿è¡ŒåŒº ---
    train_file = os.path.join(DATA_PATH, 'trainp2x_data.pkl')
    if not os.path.exists(train_file):
        print(f"é”™è¯¯ï¼šæ‰¾ä¸åˆ°è®­ç»ƒæ•°æ®æ–‡ä»¶: '{train_file}'")
    else:
        train_icu_model(
            data_path=DATA_PATH,
            epochs=EPOCHS,
            batch_size=BATCH_SIZE,
            learning_rate=LEARNING_RATE,
            modality_dropout_rate=MODALITY_DROPOUT_RATE,
            save_dir=SAVE_DIR,
            bert_model_name=BERT_MODEL_NAME,
            use_subset=USE_SUBSET_MODE,
            use_amp=USE_AMP,
            freeze_backbone=FREEZE_BACKBONE
        )