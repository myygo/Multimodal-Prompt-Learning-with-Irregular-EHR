#!/usr/bin/env python3
"""
[æœ€ç»ˆé€‚é…ç‰ˆ] ç”¨äºè®­ç»ƒ my_model.py ä¸­çš„ ICUPromptModelã€‚
å®ç°äº†ä¸åŸå§‹ train.py ä¸€è‡´çš„ç²¾ç»†åŒ–å¾®è°ƒç­–ç•¥ï¼Œå¹¶åŠ å…¥äº†æ¢¯åº¦ç´¯ç§¯ä»¥è§£å†³æ˜¾å­˜é—®é¢˜ã€‚
"""
from transformers import AutoModel, AutoTokenizer, AdamW, get_linear_schedule_with_warmup
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from sklearn.metrics import accuracy_score, f1_score, average_precision_score
from torch.utils.data import DataLoader
import os
import time
from tqdm import tqdm
import warnings

warnings.filterwarnings('ignore')

from my_model import create_icu_model
from data import data_perpare

class ICUTrainer:
    """å°è£…äº†è®­ç»ƒã€éªŒè¯å’Œæ£€æŸ¥ç‚¹ä¿å­˜çš„é€»è¾‘ï¼Œå¹¶é›†æˆäº†AMP"""
    def __init__(self, model, device, optimizer, scheduler, class_weights=None, use_amp=True):
        self.model = model.to(device)
        self.device = device
        self.optimizer = optimizer
        self.scheduler = scheduler
        self.use_amp = use_amp and torch.cuda.is_available()

        self.update_param_count()
        
        if class_weights is not None:
            class_weights = class_weights.to(device)
            print(f"æ­£åœ¨ä½¿ç”¨å¸¦æƒé‡çš„æŸå¤±å‡½æ•°ï¼Œæƒé‡ä¸º: {class_weights.cpu().numpy()}")
        self.criterion = nn.CrossEntropyLoss(weight=class_weights)
        
        if self.use_amp:
            self.scaler = torch.cuda.amp.GradScaler()
            print("  > è‡ªåŠ¨æ··åˆç²¾åº¦è®­ç»ƒ (AMP) å·²å¯ç”¨ã€‚")
            
    def update_param_count(self):
        params_to_train = [p for p in self.model.parameters() if p.requires_grad]
        print(f"  > æ¨¡å‹æ€»å‚æ•°é‡: {sum(p.numel() for p in self.model.parameters()):,}")
        print(f"  > å½“å‰å¯è®­ç»ƒå‚æ•°é‡: {sum(p.numel() for p in params_to_train):,}")

    # --- æ ¸å¿ƒä¿®æ”¹ï¼šæ·»åŠ  gradient_accumulation_steps å‚æ•° ---
    def train_epoch(self, dataloader, gradient_accumulation_steps=1):
        self.model.train()
        total_loss = 0
        progress_bar = tqdm(dataloader, desc="Training")
        
        # å°† zero_grad ç§»åˆ°å¾ªç¯å¤–
        self.optimizer.zero_grad()
        
        for step, batch_data in enumerate(progress_bar):
            if batch_data is None: continue
            
            (ts_input_sequences, ts_mask_sequences, ts_tt, reg_ts_input,
             input_ids, attn_mask, note_time, note_time_mask, label) = [
                d.to(self.device) if torch.is_tensor(d) else None for d in batch_data
            ]

            with torch.cuda.amp.autocast(enabled=self.use_amp):
                outputs = self.model(
                    ts_input_sequences=ts_input_sequences, ts_mask_sequences=ts_mask_sequences, ts_tt=ts_tt,
                    reg_ts_input=reg_ts_input, input_ids=input_ids, attn_mask=attn_mask,
                    note_time=note_time, note_time_mask=note_time_mask, label=label
                )
                if outputs is None: continue
                loss = self.criterion(outputs, label)
                
                # å¯¹æŸå¤±è¿›è¡Œç¼©æ”¾
                if gradient_accumulation_steps > 1:
                    loss = loss / gradient_accumulation_steps

            if self.use_amp:
                self.scaler.scale(loss).backward()
            else:
                loss.backward()
            
            # ç´¯åŠ åŸå§‹å¤§å°çš„æŸå¤±
            total_loss += loss.item() * gradient_accumulation_steps
            
            # --- æ ¸å¿ƒä¿®æ”¹ï¼šæ¯ N æ­¥æ›´æ–°ä¸€æ¬¡ ---
            if (step + 1) % gradient_accumulation_steps == 0 or (step + 1) == len(dataloader):
                if self.use_amp:
                    self.scaler.step(self.optimizer)
                    self.scaler.update()
                else:
                    self.optimizer.step()
                
                self.scheduler.step()
                self.optimizer.zero_grad()

            progress_bar.set_postfix({'Loss': f'{loss.item() * gradient_accumulation_steps:.4f}'}, refresh=True)

        avg_loss = total_loss / len(progress_bar) if len(progress_bar) > 0 else 0
        return avg_loss

    def validate(self, dataloader):
        self.model.eval()
        all_preds, all_labels, all_probs = [], [], []
        
        with torch.no_grad():
            for batch_data in tqdm(dataloader, desc="Validating"):
                if batch_data is None: continue
                
                (ts_input_sequences, ts_mask_sequences, ts_tt, reg_ts_input,
                 input_ids, attn_mask, note_time, note_time_mask, label) = [
                    d.to(self.device) if torch.is_tensor(d) else None for d in batch_data
                ]

                outputs = self.model(
                    ts_input_sequences=ts_input_sequences, ts_mask_sequences=ts_mask_sequences, ts_tt=ts_tt,
                    reg_ts_input=reg_ts_input, input_ids=input_ids, attn_mask=attn_mask,
                    note_time=note_time, note_time_mask=note_time_mask, label=label
                )
                
                if outputs is None: continue
                probs = torch.softmax(outputs, dim=1)[:, 1]
                predicted = torch.argmax(outputs, dim=1)
                
                all_preds.extend(predicted.cpu().numpy())
                all_labels.extend(label.cpu().numpy())
                all_probs.extend(probs.cpu().numpy())
        
        if all_labels:
            f1 = f1_score(all_labels, all_preds, average='binary')
            auprc = average_precision_score(all_labels, all_probs)
            return f1, auprc
        return 0.0, 0.0

    def save_checkpoint(self, filepath, epoch, is_best=False):
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        checkpoint = {'epoch': epoch, 'model_state_dict': self.model.state_dict()}
        if is_best:
            best_path = filepath.replace(f'_epoch_{epoch}.pth', '_best.pth')
            torch.save(checkpoint, best_path)
            print(f"Best model saved: {best_path}")

def train_icu_model(data_path, epochs, batch_size, grad_accum_steps, learning_rate, backbone_lr, fine_tune_epochs, save_dir, bert_model_name, use_amp):
    print("å¼€å§‹è®­ç»ƒ ICUPromptModel (ä½¿ç”¨ç²¾ç»†åŒ–å¾®è°ƒç­–ç•¥)")
    print("=" * 60)
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    torch.manual_seed(42); np.random.seed(42)
    
    class DataArgs:
        def __init__(self):
            self.file_path = data_path
            self.train_batch_size, self.eval_batch_size = batch_size, batch_size
            self.debug = False; self.max_length = 1024; self.num_of_notes = 5
            self.tt_max = 48; self.pad_to_max_length = True; self.notes_order = "Last"
            self.modeltype = "TS_Text"; self.model_name = bert_model_name
            self.chunk = False; self.ratio_notes_order = None
    
    data_args = DataArgs()
    tokenizer = AutoTokenizer.from_pretrained(bert_model_name)
    train_dataset, _, train_dataloader = data_perpare(data_args, 'train', tokenizer)
    _, _, val_dataloader = data_perpare(data_args, 'val', tokenizer)
    
    class_weights = torch.FloatTensor([0.57515432, 3.82648871])
    
    print("\n[æ­¥éª¤ 2] åˆ›å»ºæ¨¡å‹ (Longformer åˆå§‹ä¸ºå†»ç»“çŠ¶æ€)...")
    clinical_bert_model = AutoModel.from_pretrained(bert_model_name)
    # åˆå§‹åˆ›å»ºæ—¶ï¼Œæ€»æ˜¯å…ˆå†»ç»“
    model = create_icu_model(clinical_bert_model, freeze_backbone=True)
    
    trainer = None # trainer å°†åœ¨å¾ªç¯å†…éƒ¨åˆå§‹åŒ–å’Œæ›´æ–°
    
    best_val_metric_sum, best_f1, best_auprc, best_epoch = 0, 0, 0, 0
    patience, patience_counter = 8, 0
    
    print(f"\n[æ­¥éª¤ 5] å¼€å§‹è®­ç»ƒï¼Œå…± {epochs} ä¸ª epoch...")
    
    for epoch in range(epochs):
        print(f"\nEpoch {epoch+1}/{epochs}")
        start_time = time.time()
        
        # --- æ ¸å¿ƒä¿®æ”¹ï¼šåœ¨æ¯ä¸ª epoch å¼€å§‹æ—¶å†³å®šæ˜¯å¦å¾®è°ƒ ---
        is_fine_tuning_epoch = epoch < fine_tune_epochs

        if is_fine_tuning_epoch:
            print(f"** [ç­–ç•¥] æœ¬ Epoch ({epoch+1}) å°†å¾®è°ƒ Longformer **")
            # è§£å†»
            for param in model.text_encoder.bert.parameters():
                param.requires_grad = True
            
            # åˆ›å»ºå¸¦å·®åˆ†å­¦ä¹ ç‡çš„ä¼˜åŒ–å™¨
            optimizer_grouped_parameters = [
                {"params": model.text_encoder.bert.parameters(), "lr": backbone_lr},
                {"params": [p for n, p in model.named_parameters() if 'text_encoder.bert' not in n and p.requires_grad], "lr": learning_rate}
            ]
            optimizer = AdamW(optimizer_grouped_parameters, weight_decay=1e-4)
        else:
            if epoch == fine_tune_epochs: # åªåœ¨ç¬¬ä¸€æ¬¡è½¬æ¢æ—¶æ‰“å°
                print(f"** [ç­–ç•¥] ä»æœ¬ Epoch ({epoch+1}) å¼€å§‹ï¼Œå†»ç»“ Longformer **")
            # å†»ç»“
            for param in model.text_encoder.bert.parameters():
                param.requires_grad = False
            
            # åˆ›å»ºåªåŒ…å«ééª¨å¹²å‚æ•°çš„ä¼˜åŒ–å™¨
            params_to_optimize = [p for p in model.parameters() if p.requires_grad]
            optimizer = AdamW(params_to_optimize, lr=learning_rate, weight_decay=1e-4)

        # æ¯æ¬¡ä¼˜åŒ–å™¨å˜åŒ–åï¼Œéƒ½éœ€è¦é‡æ–°åˆ›å»ºè°ƒåº¦å™¨å’Œè®­ç»ƒå™¨
        if trainer is None or is_fine_tuning_epoch != (epoch-1 < fine_tune_epochs):
            print("  > ä¼˜åŒ–ç­–ç•¥æ”¹å˜ï¼Œé‡æ–°åˆå§‹åŒ– Trainer...")
            num_training_steps = epochs * len(train_dataloader)
            num_warmup_steps = int(0.1 * num_training_steps) # é¢„çƒ­åªåœ¨å¼€å§‹æ—¶æœ‰æ•ˆ
            scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)
            trainer = ICUTrainer(model, device, optimizer, scheduler, class_weights=class_weights, use_amp=use_amp)
        else:
             # å¦‚æœç­–ç•¥ä¸å˜ï¼Œåªéœ€æ›´æ–°ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
             trainer.optimizer = optimizer
             trainer.scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=(epochs-epoch)*len(train_dataloader))
        
        train_loss = trainer.train_epoch(train_dataloader, gradient_accumulation_steps=grad_accum_steps)
        val_f1, val_auprc = trainer.validate(val_dataloader)
        
        print(f"è®­ç»ƒ - å¹³å‡æŸå¤±: {train_loss:.4f}")
        print(f"éªŒè¯ - F1åˆ†æ•°: {val_f1:.4f}, AUPRC: {val_auprc:.4f}")
            
        current_metric_sum = val_f1 + val_auprc
        if current_metric_sum > best_val_metric_sum:
            best_val_metric_sum = current_metric_sum
            best_f1, best_auprc, best_epoch = val_f1, val_auprc, epoch + 1
            patience_counter = 0
            print(f"ğŸ‰ æ–°çš„æœ€ä½³æŒ‡æ ‡: F1+AUPRC = {best_val_metric_sum:.4f}")
            is_best = True
        else:
            patience_counter += 1
            print(f"éªŒè¯é›†æŒ‡æ ‡æ— æå‡ ({patience_counter}/{patience})")
            is_best = False
            
        checkpoint_path = os.path.join(save_dir, f"icu_model_epoch_{epoch+1}.pth")
        trainer.save_checkpoint(checkpoint_path, epoch + 1, is_best=is_best)
            
        if patience_counter >= patience:
            print(f"æ—©åœï¼šæŒ‡æ ‡è¿ç»­ {patience} ä¸ªepochæ— æå‡ã€‚")
            break
        
        print(f"Epochè€—æ—¶: {time.time() - start_time:.2f}ç§’")

    print("\n" + "="*60)
    print("è®­ç»ƒå®Œæˆæ€»ç»“:")
    if best_epoch > 0:
        print(f"æœ€ä½³éªŒè¯é›†æŒ‡æ ‡ (åœ¨ Epoch {best_epoch}):")
        print(f"  - F1 Score: {best_f1:.4f}")
        print(f"  - AUPRC: {best_auprc:.4f}")
        best_model_path = os.path.join(save_dir, 'icu_model_best.pth')
        print(f"æœ€ä½³æ¨¡å‹æƒé‡å·²ä¿å­˜è‡³: {best_model_path}")
    else:
        print("æœªèƒ½åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ‰¾åˆ°æ›´ä¼˜çš„æ¨¡å‹ã€‚")
    print("=" * 60)

if __name__ == "__main__":
    DATA_PATH = "Data/ihm"
    BERT_MODEL_NAME = "yikuan8/Clinical-Longformer"
    SAVE_DIR = "my_model_weights_finetuned"
    
    EPOCHS = 25
    
    # --- æ ¸å¿ƒä¿®æ”¹ï¼šå¼•å…¥æ¢¯åº¦ç´¯ç§¯é…ç½® ---
    EFFECTIVE_BATCH_SIZE = 32
    PHYSICAL_BATCH_SIZE = 2  # è®¾ç½®ä¸€ä¸ªéå¸¸å°çš„ç‰©ç†æ‰¹æ¬¡
    GRAD_ACCUM_STEPS = EFFECTIVE_BATCH_SIZE // PHYSICAL_BATCH_SIZE
    
    print(f"å°†ä½¿ç”¨æœ‰æ•ˆæ‰¹æ¬¡: {EFFECTIVE_BATCH_SIZE} (ç‰©ç†æ‰¹æ¬¡: {PHYSICAL_BATCH_SIZE}, ç´¯ç§¯æ­¥æ•°: {GRAD_ACCUM_STEPS})")
    
    LEARNING_RATE = 5e-5       
    BACKBONE_LR = 2e-5         
    FINE_TUNE_EPOCHS = 3       
    
    USE_AMP = True
    
    train_icu_model(
        data_path=DATA_PATH,
        epochs=EPOCHS,
        batch_size=PHYSICAL_BATCH_SIZE, # <--- ä¼ é€’ç‰©ç†æ‰¹æ¬¡
        grad_accum_steps=GRAD_ACCUM_STEPS, # <--- ä¼ é€’ç´¯ç§¯æ­¥æ•°
        learning_rate=LEARNING_RATE,
        backbone_lr=BACKBONE_LR,
        fine_tune_epochs=FINE_TUNE_EPOCHS,
        save_dir=SAVE_DIR,
        bert_model_name=BERT_MODEL_NAME,
        use_amp=USE_AMP,
    )
